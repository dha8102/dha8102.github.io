---
layout: post
title: F1 Score & Confusion Matrix
categories : [Machine learning]
tags : [Machine learning]

---

<span style = "line-height:50%"><br></span>

머신 러닝으로 특정 주제를 분류하는 모형 공부를 하다 보면 (특히 논문을 보다 보면) 모델이 정답을 얼마나 잘 맞추는지에 대한 검증 방식이 필요한데, 그 때 사용되는 검증 방식 중의 하나가 <b>F1 Score</b>이다.

왜 F1 이라는 이름이 붙었는지는 <a href = "https://en.wikipedia.org/wiki/F1_score">wiki</a>를 참조하면 되겠다. (다른 F함수의 이름을 차용했다는거 같음)

F1 Score를 이해하기 위해서는 혼동행렬부터 알아야 한다.



## [혼동행렬(Confusion Matrix)]

특정 모델로 참/거짓의 이진 분류를 한다고 가정했을 시, 모델의 분류 결과값들은 다음과 같은 표에 넣을 수 있다 :

|          | 실제 T | 실제 F |
| :------: | :----: | :----: |
| T로 예측 |   a    |   b    |
| F로 예측 |   c    |   d    |

이렇게 생긴 표를 혼동행렬이라고 부른다. 이 예시는 가장 간단한 이진 분류를 기준으로 작성되어서 2X2의 작은 크기를 가지고 있지만, 분류 결과가 여러 가지일 경우는 행렬의 크기도 그만큼 늘어난다. 기본적으로 혼동행렬은 행과 열의 개수가 같다 (그럴 수밖에 없다).

이 행렬에서 a는 실제 T인 결과를 모델이 T로 정확히 예측한 것이고 b는 실제는 F인데 T로 예측한 결과, c는 실제 T인데 F로 예측한 결과, d는 실제 F인데 F로 예측한 결과를 의미한다.

<b>[정확도(Accuracy)]</b>

정확도는 전체에서 예측을 정확히 한 데이터의 비율을 나타낸다.

$ Accuracy = (a+d) / (a+b+c+d)$

<b>[정밀도(Precision)]</b>

정밀도는 T로 예측한 것들 중 실제 T인 데이터의 비율을 나타낸다. T로 예측한 것이 얼마나 정확하게 맞았는지를 나타낸다. Positive Predictive Value라고도 한다.

$ Precision = a / (a+b)$

<b>[Negative Predictive Value]</b>

이 값은 정밀도의 반대인데, 한국어로 정확한 번역이 없는 것 같다. (정확히는 업계별로 부르는 명칭이 다른 듯 하다)

F로 예측한 것들 중 실제 F인 데이터의 비율을 나타낸다. F로 예측한 것이 얼마나 정확하게 맞았는지를 나타낸다.

$ Negative Predictive Value = d / (c+d) $

<b>[민감도, 재현율(Sensitivity, Recall, True Positive Rate)]</b>

중요한 수치라서 그런지 부르는 말이 많다. 민감도와 재현율 둘 다 자주 쓰는 말이라 둘 다 알아두는 것이 좋다.

실제 T인 데이터들 중 T로 예측한 데이터의 비율을 나타낸다.

$ Sensitivity(Recall) = a / (a+c) $

<b>[특이도(Specificity)]</b>

민감도의 반대 개념이다. 실제 F인 데이터들 중 F로 예측한 데이터의 비율을 나타낸다.

$ Specificity = d / (b+d) $

모델을 평가할 때는 위 지표들 중 하나만 높다고 해서 좋은 모델이 아니고, 여러 가지 지표들의 수치를 종합해서 살펴야 한다. 학계/업계에 따라, task에 따라 중요하게 여겨지는 지표가 다르므로 각 상황에 맞게 지표를 활용할 것.





<span style = "line-height:50%"><br></span>

## [F1 Score]

위 지표들을 하나의 숫자로 표현해서 모델을 평가할 수 없을까? 라는 생각에서 나온 지표가 F-Score이다.  F-Score는 Precision과 Recall의 조화평균인데, 그냥 평균을 내면 값의 왜곡이 생길 수 있기 때문에 가중치 지표 $ \beta $를 추가해 사용한다.

$ F_{\beta} = \frac{(1+\beta^{2})(Precision\times Recall)}{(\beta^{2}\times Precision + Recall)} $

여기에서 $ \beta $가 1일 경우를 F1 Score라고 한다.

$ F_{1} = 2\times \frac{(Precision\times Recall)}{(Precision + Recall)} $

많은 논문에서 모델의 성능을 측정하기 위해 사용되는 지표이다.

<span style = "line-height:50%"><br></span>

---

F1-Score를 구하기 위해 Precision과 Recall을 구해야 할 때 다음과 같은 문제가 발생할 수 있다.

Precision과 Recall의 식은 다음과 같은데,

$ Precision = a / (a+b)$

$ Sensitivity(Recall) = a / (a+c) $

이 때 분모가 0이 되는 경우가 생길 수가 있다. 그래서 종종 프로그래밍을 하다 보면 Zero-Divion Error가 날 때가 있다. 이는 무슨 의미일까?

- Precision이 0이 되는 경우

  Precision의 분모인 a+b가 0이 되는 경우는 모든 예측값이 negative로 예측될 때 발생한다.

- Recall이 0이 되는 경우

  Recall의 분모인 a+c가 0이 되는 경우는 input data 자체에 positive한 값이 없을 때 발생한다. 데이터가 완전히 한쪽에만 있다는 얘기이다.

두 경우 다 이진 분류에서는 거의 일어날 수 없는 일이다. 하지만 클래스가 여러 개일 때(Multi-Class Classify)에는 특정 class의 Precision의 값이 0이 되는 경우가 왕왕 일어난다. 이 때는 모델의 성능을 조금 더 높여서 그런 class가 없게 만들던지, 해당 클래스를 분석에서 제외하는 등의 방법을 생각해볼 수 있다.