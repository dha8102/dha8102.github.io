---
layout : post
title : TF-IDF
categories : [NLP]
tags : [NLP]
---

---

<span style = "line-height:50%"><br></span>

TF-IDF는 Term frequency - Inverse Document Frequency의 약자로, 문서에 나온 단어의 개수와 관련된 모델이다. 보통 이 개념을 사용한 Vector Matrix를 생성하여 기초적인 검색에 활용한다.

TF는 특정 단어가 문서에서 얼마나 자주 등장하는지 빈도를 나타내고, DF는 단어 자체가 전체 문서들에서 얼마나 자주 나타내는지를 뜻한다. IDF는 DF의 역수 값이다. TF-IDF값은 TF값에 IDF를 곱해서 나타낸다.

간단한 예시로 계산을 진행하면,

|d1 : This picture is my favorite.|

|d2 : I love this contury.|

|d3 : Choose your favorite picture.|

위와 같이 3개의 문서가 있다고 가정할 때,

각 단어의 빈도수를 표로 나타내면 다음과 같다 :

|        | this | picture |  is  |  my  | favorite |  I   | love | contury | choose | your |
| :----: | :--: | :-----: | :--: | :--: | :------: | :--: | :--: | :-----: | :----: | :--: |
|   d1   |  1   |    1    |  1   |  1   |    1     |  0   |  0   |    0    |   0    |  0   |
|   d2   |  1   |    0    |  0   |  0   |    0     |  1   |  1   |    1    |   0    |  0   |
|   d3   |  0   |    1    |  0   |  0   |    1     |  0   |  0   |    0    |   1    |  1   |
| **TF** |  2   |    2    |  1   |  1   |    2     |  1   |  1   |    1    |   1    |  1   |

(한 문서 안에 같은 단어가 여러 번 나오면 그에 따라 빈도수도 올라간다)

각 단어별 TF는 위와 같이 단어가 모든 문서군에서 총 몇 번 나왔는지를 세 주면 된다.

IDF는 문서의 수(여기서는 3개니까 3)를 단어의 빈도로 나눠주고 거기에 밑을 2로 하는 log를 취해 주면 된다.

그러니까 this의 IDF는 $<a href="https://www.codecogs.com/eqnedit.php?latex=IDF_{this}&space;=&space;log_{2}(3/2)&space;=&space;0.584" target="_blank"><img src="https://latex.codecogs.com/gif.latex?IDF_{this}&space;=&space;log_{2}(3/2)&space;=&space;0.584" title="IDF_{this} = log_{2}(3/2) = 0.584" /></a>$ 가 된다.

만약 단어의 빈도수가 0일 경우, 0으로 나누는 문제를 방지하기 위해서 분모에 1을 더해서 계산하기도 한다. 

(이렇게 해야 되는 이유에 대해서는 조금 더 자세한 탐색이 필요하다.)

위의 표의 마지막 열에 IDF를 추가하면 다음과 같이 된다 :

|         | this  | picture |  is   |  my   | favorite |   I   | love  | contury | choose | your  |
| ------- | :---: | :-----: | :---: | :---: | :------: | :---: | :---: | :-----: | :----: | :---: |
| d1      |   1   |    1    |   1   |   1   |    1     |   0   |   0   |    0    |   0    |   0   |
| d2      |   1   |    0    |   0   |   0   |    0     |   1   |   1   |    1    |   0    |   0   |
| d3      |   0   |    1    |   0   |   0   |    1     |   0   |   0   |    0    |   1    |   1   |
| **TF**  |   2   |    2    |   1   |   1   |    2     |   1   |   1   |    1    |   1    |   1   |
| **IDF** | 0.584 |  0.584  | 1.584 | 1.584 |  0.584   | 1.584 | 1.584 |  1.584  | 1.584  | 1.584 |

이제 TF-IDF 값은 단순히 TF와 IDF 두 값만 곱해서 표현하면 된다.

추가적으로 나중의 문서 유사도 검색을 위해서 문서의 크기를 알아야 하는데, 문서의 크기(length)는 문서의 모든 단어들에 등장하는 TF-IDF 값들의 제곱의 합의 L2-norm으로 구할 수 있다.

위의 표의 d1~d3 행을 TF-IDF 값으로 표현하고 length를 추가하면,

|      | this  | picture |  is   |  my   | favorite |   I   | love  | contury | choose | your  | length |
| :--: | :---: | :-----: | :---: | :---: | :------: | :---: | :---: | :-----: | :----: | :---: | :----: |
|  d1  | 1.168 |  1.168  | 1.584 | 1.584 |  1.168   |   0   |   0   |    0    |   0    |   0   | 3.018  |
|  d2  | 1.168 |    0    |   0   |   0   |    0     | 1.584 | 1.584 |  1.584  |   0    |   0   | 2.981  |
|  d3  |   0   |  1.168  |   0   |   0   |  1.168   |   0   |   0   |    0    | 1.584  | 1.584 | 2.783  |

이 표를 벡터 매트릭스로 해석하고 새로운 문서와 유사도를 측정할 수 있다.