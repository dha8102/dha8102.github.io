---
layout: post
title: 언어 모델과 N-gram
categories : [NLP]
tags : [NLP, Machine learning]
---

---

<span style = "line-height:50%"><br></span>

## [언어 모델(Statistical Language Model)]

NLP 분야에서는 언어를 컴퓨터가 이해하기 쉽도록 개발된 여러 가지 모델이 존재하는데, 이를 일컬어 언어 모델이라고 한다. 각기 다른 방식을 사용하지만, 기본적으로 모든 언어 모델은 다음과 같은 질문에 답하기 위한 모델이다 :

1) 해당 문장이 나올 확률이 얼마인가?

2) 이전 단어들이 주어졌을 때, 다음 단어가 나올 확률이 얼마인가?

이 두 질문에 답을 하는 언어 모델들을 통해 단어들의 조합이 적절한지, 단락 내의 해당 문장이 얼마나 적합한지 등을 알아낼 수 있다.

<span style = "line-height:50%"><br></span>



그렇다면 w1 ~ w5 5개의 연속된 단어로 이루어진 문장 W가 있다고 가정해보자. 이 때 W가 나타날 확률은

$P(W) = P(w1,w2,w3,w4,w5)$

로 나타낼 수 있다.

그럼 P(w1), P(w2) 같은 것들은 어떻게 알 수 있는가?

기본적으로 자연어 처리에서의 단어가 나올 확률은 이전 단어가 나왔을 때 그 다음 해당 단어가 나올 조건부 확률로 정의된다. 그러니까 

<a href="https://www.codecogs.com/eqnedit.php?latex=P(w2)&space;=&space;P(w2|w1)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(w2)&space;=&space;P(w2|w1)" title="P(w2) = P(w2|w1)" /></a> 

으로 표현할 수 있단 말.



이 조건부확률들을 joint하면 P(W)도 알아낼 수 있다.

<a href="https://www.codecogs.com/eqnedit.php?latex=P(W)&space;=&space;P(w5|w1,w2,w3,w4)&space;=&space;P(w5|P(w4|w1,w2,w3))&space;=&space;..." target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(W)&space;=&space;P(w5|w1,w2,w3,w4)&space;=&space;P(w5|P(w4|w1,w2,w3))&space;=&space;..." title="P(W) = P(w5|w1,w2,w3,w4) = P(w5|P(w4|w1,w2,w3)) = ..." /></a>

이렇게 표현할 수 있다. 조금 더 세련되게 표현하면

<a href="https://www.codecogs.com/eqnedit.php?latex=P(W)&space;=&space;\prod_{i}P(w_{i}|w_{1},w_{2},w_{3},...w_{i-1})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?P(W)&space;=&space;\prod_{i}P(w_{i}|w_{1},w_{2},w_{3},...w_{i-1})" title="P(W) = \prod_{i}P(w_{i}|w_{1},w_{2},w_{3},...w_{i-1})" /></a>

이런 조건부 확률을 계산하려면 엄청나게 많은 양의 코퍼스가 있어야 하겠다.

그런데 문제, 해당하는 단어나 문장이 코퍼스 안에 아예 없다면? 아예 확률 계산 자체가 불가능해지는 문제가 생긴다.

이를 일반화하여 조금이라도 개선하는 방법론이 N-gram이다.



## [N-gram]

N-gram이란, 입력된 문자열을 N개의 기준 단위로 자르는 방법이다. 이 때 기준 단위는 문자 단위가 될 수도, 단어 단위가 될 수도, 어절 단위가 될 수도 있다. (정하기 나름이라는 거다)

위에서 기술한 조건부 확률을 이용한 방법은 코퍼스에 해당하는 문장이 없으면 아예 카운팅이 안 되지만, N-gram을 이용하면  문장 전체가 없어도 부분으로 쪼개서 탐색하기 때문에 이런 경우가 줄어든다.

ex) "I am a boy"를 단어 단위 2-gram으로 자르면,

\{"I am", "am a", "a boy"\} 의 집합으로 나타내어진다.

이 기법은 빈도 분석, 검색, 자연어 처리 등에 널리 이용된다.

프로그램은 문자열 입력 - N-gram 만들기 - 빈도순 출력 순으로 만들면 된다.

<b>[적절한 n의 개수 정하기]</b>

당연하게도, n을 크게 잡으면 정확도가 높아지지만 실제 훈련 코퍼스에서 해당 데이터를 count할 수 있는 확률은 적어진다. n을 작게 잡는다면 count 자체는 잘 되겠지만 근사의 정확도는 점점 실제의 분포와 멀어진다. 적절한 n을 선택하는 것 또한 하나의 주제가 될 수 있다. 

