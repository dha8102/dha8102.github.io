---
layout: post
title: word2vec에 대하여
categories : [NLP, Deep learning]

tags : [NLP, Deep learning, cs224n, word2vec]

---

<span style = "line-height:50%"><br></span>



## [단어 임베딩]

텍스트 데이터를 컴퓨터가 인식할 수 있게 바꿔주는 방법 중 하나가 임베딩(Embedding)인데, 말이 어렵지 그냥 단어(Word)를 벡터(Vector)로 변환시키는 방법이다. 단어들을 임베딩하는 방법들은 여러 가지가 있는데, 그 중 가장 쉽게 생각할 수 있는 방법은 주어진 텍스트에 있는 단어 수만큼의 차원을 만들어 one-hot-encoding으로 만드는 방식이다. 이 방법은 자연어 처리 초창기에 등장한 NNLM(Neural Network Language Model)은 이러한 방식으로 벡터를 만들어 임베딩을 하였지만, one-hot-encoding이었기 때문에 비슷한 단어들끼리의 유사성을 나타내기 어려웠고, 또 너무 많은 차원 수가 필요했으므로 매우 느렸다. (자세한 내용은 <a href = "https://shuuki4.wordpress.com/2016/01/27/word2vec-%EA%B4%80%EB%A0%A8-%EC%9D%B4%EB%A1%A0-%EC%A0%95%EB%A6%AC/">  Beomsu Kim님의 블로그 </a>로)

## [word2vec의 등장]

그래서 등장한 것이 word2vec이다. word2vec은 2013년 구글의 Mikolov를 필두로 발표된 모델로, 논문은 <a href = "https://arxiv.org/abs/1301.3781">여기</a>에서 볼 수 있다. word2vec은 각 단어들을 "P(o\|c) : 중심단어 c가 등장했을 때 주변단어 o가 등장할 조건부 확률" 로 정의한다. 

<img src = "https://lh3.googleusercontent.com/AidFfKMU4cOfEDDOp46KG5gcKKHvYyg0Q_OnMLEq06uB63rdX_Ap9-6AhKALmRwH-cVuMjSnJEjf6OOvzAXotg3hyLKzzuR9IJv95ARuCAw-S3v4pqo_59_aie2jVGGldzHiSbzORML4pWlGS5Fg1F3_vTsSpgqXaNpF9kCInRtwBWHp-ujT08AR-CNinKAEwHhgoNNCgCVt9RC_e_a7aoAqpNr6P7V1-b-s0zuOBhdkufGDJtUOvDul0alLNqrID5bYFPhcWnQLvB7Len8n3HZzHdMQdcaQf1ghAnXFLYtjb1Czy0w3FpERTvT8dvD9YZayu86iw3JPvD_F6-ZYHoJpdT8HCYFlO4nrCfkdr4mDOK0c5lDJfrzLXKHXF3OIYHv2dgJFHW7ypOs-CEpzJWE_bkeSxIabPjJFCKuBINo13-AMPaoETwJvhplh9XBKBBihTlFuT0nNpfK76ky5ifuAKMTPrFm_tIDaOn1RdZvHJXwa_tKGi8x8Qu9Y7TUjNhn-Y1H660dMHyk7H7wI6R__kgqcH3igbgKnO5aObStTsftTM-zjUEiynE88JMbx7mBwsy_nvgEz88whAmaCfYwuJnLtAPKJtIHqw-xcJRr-TO5K5X7Queb0hSLF6roja_D9O32MqN1CYNsDx_lp8JQ=w1239-h501-no">

그림으로 나타내면 요렇다. 출처 : <a href = "http://web.stanford.edu/class/cs224n/lectures/lecture2.pdf"> cs224n 2강</a>

이 때 P(o\|c)를 수식으로 나타내면 다음과 같다 :

<a href="https://www.codecogs.com/eqnedit.php?latex=_{}P(o|c)&space;=&space;\frac{exp({u_{o}}^{T}v_{c}))}{\sum_{w=1}^{V}exp({u_{w}}^{T}v_{c})}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?_{}P(o|c)&space;=&space;\frac{exp({u_{o}}^{T}v_{c}))}{\sum_{w=1}^{V}exp({u_{w}}^{T}v_{c})}" title="_{}P(o|c) = \frac{exp({u_{o}}^{T}v_{c}))}{\sum_{w=1}^{V}exp({u_{w}}^{T}v_{c})}" /></a>



$u\_{o}$는 주변단어들의 벡터, $v\_{c}$는 중심단어의 벡터, $u\_{w}$는 말뭉치 내의 모든 단어를 나타낸다. (V의 범위는 설정하기 나름이다 -> hyperparameter) 이 값을 최대한 크게 해야 모델로부터 생성되는 벡터들의 매핑이 보다 정확하게 된다. 이 값을 크게 하려면 분자를 최대화, 분모를 최소화 시켜야 하는데, 

분자의 최대화 : 주변에 등장한 단어와 중심 단어의 내적을 크게

분모의 최소화 : 주변에 등장하지 않은 단어와 중심 단어의 내적값을 작게

요런 뜻을 가지고 있다.

이 값을 최대화시키는 방법은  해당 식을 $v_{c}$에 관하여 편미분하여 극점이 되는 부분을 찾으면 가능하다. 자세한 유도는 아래와 같다 : 

<img src = "https://imageshack.com/a/img921/9687/11ulrX.jpg" width = "600">



최종 결과 식에서 $u\_{o}$는 바꿀 수 없는 값이고(이미 observed된 것), 바꿀 수 있는 것은 minus 부호 뒤의 값들인데, 이들은 V의 범위를 조정함으로써 바꿀 수 있다. 이 식의 값을 딥 러닝 기법을 통해 최적화 하는 것이 word2vec 모델에서 쓰인 수학적 기법이라 할 수 있겠다.